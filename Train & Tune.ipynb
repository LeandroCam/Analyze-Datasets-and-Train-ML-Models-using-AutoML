{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b0bf9c",
   "metadata": {},
   "source": [
    "we ll use SageMaker BlazingText built-in algorithm to predict the sentiment for each customer review. BlazingText is a variant of FastText which is based on word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!pip install --disable-pip-version-check -q nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b511deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acff95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b132d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's adapt the dataset into a format that BlazingText understands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "You will tokenize the review_body with the Natural Language Toolkit (nltk) for the model training. \n",
    "You will also use nltk later in this lab to tokenize reviews to use as inputs to the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2509313",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './womens_clothing_ecommerce_reviews_balanced.csv'\n",
    "\n",
    "df = pd.read_csv(path, delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2b9d7",
   "metadata": {},
   "source": [
    "Now you will prepend __label__ to each sentiment value and tokenize the review body using nltk module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de1794",
   "metadata": {},
   "source": [
    "To split a sentence into tokens you can use word_tokenize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm not a fan of this product!\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c034dea",
   "metadata": {},
   "source": [
    "Let's define a prepare_data function which you will apply later to transform both training and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761eb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    # delete commas and quotation marks, apply tokenization and join back into a string separating by spaces\n",
    "    return ' '.join([str(token) for token in nltk.word_tokenize(str(review).replace(',', '').replace('\"', '').lower())])\n",
    "    \n",
    "def prepare_data(df):\n",
    "    df['sentiment'] = df['sentiment'].map(lambda sentiment : '__label__{}'.format(str(sentiment).replace('__label__', '')))\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    df['review_body'] = df['review_body'].map(lambda review : tokenize(review)) # Replace all None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ed34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample dataframe\n",
    "df_example = pd.DataFrame({\n",
    "    'sentiment':[-1, 0, 1], \n",
    "    'review_body':[\n",
    "        \"I do like this product!\", \n",
    "        \"this product is ok\", \n",
    "        \"I don't like this product!\"]\n",
    "})\n",
    "\n",
    "# test the prepare_data function\n",
    "print(prepare_data(df_example))\n",
    "\n",
    "# Expected output:\n",
    "#      sentiment                   review_body\n",
    "# 0  __label__-1      i do like this product !\n",
    "# 1   __label__0            this product is ok\n",
    "# 2   __label__1  i do n't like this product !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply the prepare_data function to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aad687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blazingtext = df[['sentiment', 'review_body']].reset_index(drop=True)\n",
    "df_blazingtext = prepare_data(df_blazingtext)\n",
    "df_blazingtext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6bf5f",
   "metadata": {},
   "source": [
    "Split and visualize a pie chart of the train (90%) and validation (10%) sets. You can do the split using the sklearn model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split all data into 90% train and 10% holdout\n",
    "df_train, df_validation = train_test_split(df_blazingtext, \n",
    "                                           test_size=0.10,\n",
    "                                           stratify=df_blazingtext['sentiment'])\n",
    "\n",
    "labels = ['train', 'validation']\n",
    "sizes = [len(df_train.index), len(df_validation.index)]\n",
    "explode = (0.1, 0)  \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78437a3",
   "metadata": {},
   "source": [
    "Save the results as CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_train_path = './train.csv'\n",
    "df_train[['sentiment', 'review_body']].to_csv(blazingtext_train_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_validation_path = './validation.csv'\n",
    "df_validation[['sentiment', 'review_body']].to_csv(blazingtext_validation_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2252c98",
   "metadata": {},
   "source": [
    "You will use these to train and validate your model. Let's save them to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34953fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_uri = sess.upload_data(bucket=bucket, key_prefix='blazingtext/data', path=blazingtext_train_path)\n",
    "validation_s3_uri = sess.upload_data(bucket=bucket, key_prefix='blazingtext/data', path=blazingtext_validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7faa33",
   "metadata": {},
   "source": [
    "Setup the BlazingText estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ca84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    region=region,\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    framework='blazingtext' # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a3a71",
   "metadata": {},
   "source": [
    "Create an estimator instance passing the container image and other instance parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    image_uri=image_uri, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size=30,\n",
    "    max_run=7200,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e8ac7",
   "metadata": {},
   "source": [
    "Configure the hyper-parameters for BlazingText. You are using BlazingText for a supervised classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279703bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(mode='supervised',   # supervised (text classification)\n",
    "                              epochs=10,           # number of complete passes through the dataset: 5 - 15\n",
    "                              learning_rate=0.01,  # step size for the  numerical optimizer: 0.005 - 0.01\n",
    "                              min_count=2,         # discard words that appear less than this number: 0 - 100                              \n",
    "                              vector_dim=300,      # number of dimensions in vector space: 32-300\n",
    "                              word_ngrams=3)       # number of words in a word n-gram: 1 - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4253d85",
   "metadata": {},
   "source": [
    "To call the fit method for the created estimator instance you need to setup the input data channels. This can be organized as a dictionary\n",
    "Create a train data channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df675f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    train_s3_uri,# Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e086b",
   "metadata": {},
   "source": [
    "Create a validation data channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    validation_s3_uri, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e480748",
   "metadata": {},
   "source": [
    "Organize the data channels defined above as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d472ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    'train': train_data, # Replace None\n",
    "    'validation': validation_data # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72465554",
   "metadata": {},
   "source": [
    "Start fitting the model to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae08ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    inputs=data_channels, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7591705",
   "metadata": {},
   "source": [
    "Review the train and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.training_job_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a0ebb",
   "metadata": {},
   "source": [
    "Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text_classifier = estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m5.large',\n",
    "                                   serializer=sagemaker.serializers.JSONSerializer(),\n",
    "                                   deserializer=sagemaker.deserializers.JSONDeserializer())\n",
    "\n",
    "print()\n",
    "print('Endpoint name:  {}'.format(text_classifier.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d78a56",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb775c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61f9d1",
   "metadata": {},
   "source": [
    "Specify sample reviews to predict the sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea95ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['This product is great!',\n",
    "           'OK, but not great',\n",
    "           'This is not the right product.'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3c10a",
   "metadata": {},
   "source": [
    "Tokenize the reviews and specify the payload to use when calling the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = [' '.join(nltk.word_tokenize(review)) for review in reviews]\n",
    "\n",
    "payload = {\"instances\" : tokenized_reviews}\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f8e53",
   "metadata": {},
   "source": [
    "Now you can predict the sentiment for each review. Call the predict method of the text classifier passing the tokenized sentence instances (payload) into the data argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(data=payload)\n",
    "for prediction in predictions:\n",
    "    print('Predicted class: {}'.format(prediction['label'][0].lstrip('__label__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba88af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574cf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d000f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db313e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330e928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61968edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d90ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
